# Smooth migration to Kubernetes

## Summary

This document presents a design to migrate TiDB cluster deployed in PM/VM to Kubernetes without using migration tools.

## Motivation

Users may want to migrate TiDB deployed in PM/VM to Kubernetes by creating a new TiDB in Kubernetes which join in the existing 
PD cluster.

### Goals

* Migrate existing TiDB cluster to Kubernetes by creating a new one in Kubernetes specified PD addresses.

### Non-Goals

* Only support PD addresses starting with `HTTP` scheme at present.
* If the new TiDB cluster created in Kubernetes can not connect to the PD addresses of existing TiDB cluster, the cluster can't be created successfully.

## Proposal

## Design Details

Users created a TiDB cluster in Kubernetes by specifying the `TidbCluster.spec.PDAddresses`, whose format is an array of `{scheme}:\\{address}:{port}`.
The `PDAddresses` is the PD addresses of the cluster you want migrate data from. If the addresses are right, and the PD in Kubernetes can 
connect to one of them, the new PD will join in it successfully, after that, the new cluster can be created successfully. TiKV `region` will be migrated automatically by raft 
to new TiKV Pods. After you scale in TiKV deployed in PM/VM, all data will be migrated to new TiKV automatically.

### Test Plan

1. Creating a TiDB cluster in old version tidb-operator, and create 8 tables, each 1000000 rows.
2. Upgrading tidb-operator to new version with spec.PDAddress. The existing TiDB cluster is running without influenced.
3. Creating a tidb cluster with spec.PDAddress specified(If the format not matches regex of ^http://.*:[0-9]+$, it won't be created).
4. Transferring PD leaders to the new TiDB cluster, and scaling in PD replicas to 0. It can still work normally.
5. Deleting all the stores in the old TiDB cluster, and then deleting the old cluster.
6. Notice that all the data are transferred successfully.
7. Removing the spec.PDAddress or not has no effect to the cluster.

